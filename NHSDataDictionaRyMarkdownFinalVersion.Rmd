---
title: "NHSDataDictionaRy Workshop"
subtitle: "NHS-R Community"
author: "Gary Hutson - Senior Data Scientist"
date: "04/10/2021"
output:
  
  xaringan::moon_reader:
    includes: 
      after_body: 
        - html/insert-logo.html
    css:
      - default
      - css/mango.css
      - css/mango-fonts.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      seal: false
      self_contained: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
  
---

``` {r setup, include=FALSE}
library(tidyverse)
library(NHSDataDictionaRy)
#devtools::install_github("gadenbuie/xaringanExtra")
library(xaringanExtra)
library(ggplot2)
library(plotly)
library(purrr)
```


## Introduction

.pull-left[
Welcome to the NHSDataDictionaRy workshop. Today we will learn how to work with the data dictionary for NHS lookup tasks, as well as how to extend the package to work with any website. The concentration of the workshop will be broken down as such:

- Getting familiar with the NHSDataDictionaRy package and all the underlying functions
- Understanding the [nhs_data_elements()](https://rdrr.io/cran/NHSDataDictionaRy/man/nhs_data_elements.html) elements function and how to filter on this
- Gather text from any website and perform some text cleaning operations on the text, using a combination of functions contained in the package
- Using the [TableR](https://rdrr.io/cran/NHSDataDictionaRy/man/tableR.html) function to retrieve HTML tables from the data dictionary site and then extending this to other websites
- Working with XPath website elements with the [NHSDataDictionaRy package](https://cran.r-project.org/web/packages/NHSDataDictionaRy/vignettes/introduction.html)

]

.pull-right[
   <a href="https://cran.r-project.org/web/packages/NHSDataDictionaRy/"><img src = "man/figures/NHSDataDict.png"></a>
]

---
## Watch our previous webinar on how to use this package
This was taken from a webinar we recorded a how to guide from the launch of the NHSDataDictionaRy package. This may be useful for referring to, but most of what is covered in the video will be in today's workshop. 
<br></br>


<iframe width="600" height="400" src="https://www.youtube.com/embed/MqCFHCbTORs" align="left" title="NHSDataDictionaRy tutorial" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


---
class: inverse, middle, left, hide-logo
# Loading libraries

---
# Loading libraries in NHSDataDictionaRy package

To load the libraries needed for this session you will need to bring in the following:

```{r comment='#'}
library(NHSDataDictionaRy)
```

This will bring in the libraries needed to work with the package. The next step would be to check the dependencies in the package: 

```{r avail_packs}
packrat:::recursivePackageDependencies("NHSDataDictionaRy", lib.loc = .libPaths())
```

---
class: inverse, middle, left, hide-logo
# Working with the TableR, nhs_data_elements and nhs_tableR functions

---
# Starting with the nhs_data_elements() function

To retrieve the list of currrent NHS lookups in the table finder we use the underlying function:

```{r table_finder}

nhs_data_table <- nhs_data_elements()
```

This function will then return the full list of data elements the package has scraped from the [NHS Data Dictionary website](https://www.datadictionary.nhs.uk/). 

An example of what has been scraped is on the next slide.

---

# Working with the scraped values

These scraped values will allow us to access the XPath elements of the website directly (don't worry if you don't know what an XPath is, we will get to that later on). To pull down a lookup table using the nhs_data_elements master reference we can use the TableR function to achieve this:

## Getting the Activity Treatment Function Codes

```{r table_r_working_with_table}

# Filter the table
act_treatment_function <- nhs_data_table %>% 
  dplyr::filter(link_name == "ACTIVITY TREATMENT FUNCTION CODE") 
```

On the next slide you will see this has returned the lookup that I require:

---

```{r lookup}
# Use the TableR function to pull back the data table
act_treat_lookup <- NHSDataDictionaRy::tableR(url=act_treatment_function$full_url,
                          xpath=act_treatment_function$xpath_nat_code,
                          title="NHS Hospital Activity Treatment Function Code")

print(head(act_treat_lookup,5))

```
This lookup will return the lookup table needed.
---
## What if the element is not returned?

I include an example of when an element is not returned, as not all elements from the list have corresponding HTML data tables to extract. A worked example is below:

```{r table_r_working_with_table_error}

# Filter the table
abb_ment_test_score <- nhs_data_table %>% 
  dplyr::filter(link_name == "ABBREVIATED MENTAL TEST SCORE") 

# This will show a NULL return, as there is no HTML table present
null_output <- NHSDataDictionaRy::tableR(abb_ment_test_score$full_url,
                          abb_ment_test_score$xpath_nat_code)


```
This shows a ***NULL*** return, as there is no HTML present for the national code specified. This can be highlighted by using the NHS Data Dictionary website to highlight this.

---
## Checking the NHS Data Dictionary website

.pull-left[
<img src="man/figures/NHSWebsiteDD1.png" alt="drawing" width="700" height="300"/>
]

.pull-right[
You can see that the website contains no national code table, thus the message.

You will not be able to retrieve these elements from the site, so the package presents you with a warning that these HTML table tables do not exist. 

This is supposed to be informative and will allow you then to quickly inspect other items of interest.

]

---
background-image: url(man/figures/practice.jpg)
# Practice time - 10 minutes
Have a go at extracting elements from the nhs_data_elements() and TableR functions.

---
# Introducing a quicker way to retrieve elements

Until now we have been working long hand and finding with retrieving the list of lookups and then using the TableR function. There is a more simple function in the package to use to do this, but I wanted to get you familiar with using the returned elements such as the URL and XPath, because we will come on to this again later.

## nhs_table_findeR to the rescue

```{r quicker_way_to_ret_elements}
tfc <- NHSDataDictionaRy::nhs_table_findeR("ACTIVITY TREATMENT FUNCTION CODE",
                                           title="Treatment Function Code")

glimpse(tfc)

```

This function replaces the convoluted code we used earlier. 

---
## Using the lookup to join on to hospital data
We have this lookup and now we will generate some hospital data to match with our lookups:
```{r generated_data}

set.seed(123)
spec_code <- rep(as.character(c(101,102,103,104,105,106,107, 108)), 12)
attends <- round(rnorm(length(spec_code), 500, 50))
breaches <- round(rnorm(length(spec_code), 40, 3))
admits <- round(rnorm(length(spec_code), 300, 20))
month <- rep(c("Jan", "Feb", "March", "April", "May", "Jun", "July", "Aug", "Sept",
               "Oct", "Nov", "Dec"),8)
# Combine these atomic vectors into tibble
ed_act_by_month <- tibble(spec_code, attends, breaches, admits, month)
glimpse(ed_act_by_month)
                          
```


---
## Using our custom data to join our lookups

Now we have some data to match, we can join our dynamic lookup from the web on to the lookup:

```{r join_on activity_data}
# Get our tfc and we are going to join this to specialty code
joined_activity = ed_act_by_month %>% 
  dplyr::left_join(tfc, by = c("spec_code"="Code")) %>% 
  dplyr::select(everything(), -c(Dict_Type, DttmExtracted))

glimpse(joined_activity)


```


---
background-image: url(man/figures/practice.jpg)
# Practice time - 10 minutes
Try joining the main specialty code to this data

---
## Practice solution
This is pretty simple really - I just need to use the nhs_table_findeR function to get my lookup, if you are not sure what the string is to pass, use the nhs_data_elements() function to get all the strings. 

```{r practice_solution_two}
main_spec <- NHSDataDictionaRy::nhs_table_findeR("CARE PROFESSIONAL MAIN SPECIALTY CODE",
                                    title="Main Specialty")

# Join our main specialty on to the ed_act_by_month tibble
main_spec_with_act <- ed_act_by_month %>% 
  dplyr::left_join(main_spec, by= c("spec_code"="Code")) %>% 
  dplyr::select(everything(), -c(Dict_Type, DttmExtracted))

glimpse(main_spec_with_act)

```


---
class: inverse, middle, left, hide-logo
# Wider use cases of the web scraping potential

---
## Scraping the Championship Football table
We could use the tableR function to scrape the results of the Championship table, here we will have to do some text processing as well to clean this up. I want to see how good or bad my team Nottingham Forest are doing.To get the Xpath I will need to use the Inspect button in google to copy this:

<img src="man/figures/football_gif.gif" alt="drawing" height="350px" width="700px"/>

---
### Using the retrieved Xpath from Google

The next step would be to copy the xpath and url into a variable, so we can differentiate them later.

```{r google_xpath_football}
football_xpath <- '//*[@id="u16945876197938725"]/div/div[2]/div/div/div[2]/div/div/table'
url <- "https://www.bbc.co.uk/sport/football/championship/table"
```

Now let's get the HTML table from the website. Important to note that if it does not end in **/table** at the end of the url then it cannot be used with the TableR function, as it is not an HTML table object. 

```{r google_football_table}
football_table <- NHSDataDictionaRy::tableR(url=url, 
                          xpath=football_xpath,
                          title="English Championship League Standings")

head(football_table, 3)

```

---
### Let's clean the table up
The first stage would be to drop some of the less informative columns:

```{r drop_cols_footy}
# Convert to integer
ftball_reduced <- football_table %>% 
  dplyr::select(-c(V1, V2, Form, Dict_Type,
                   DttmExtracted)) %>% #Purge the none informative columns
  dplyr::slice(1:nrow(football_table)-1) %>% # Get rid of the last row
  dplyr::add_tally()#Filter out team 

#Remove team
minus_team <- ftball_reduced %>% 
  dplyr::select(!contains("Team"))

# Convert all fields to integer
minus_team <- data.frame(sapply(minus_team, as.integer))
# Bind back together

football_table <- cbind(Team=ftball_reduced$Team, minus_team)

```



---
### Viewing the data on a scatter chart to explore key metrics
```{r glimpsing_the_table, out.width='100%', out.height='55%'}

plot <- ggplot(football_table, 
       aes(x=W, y=Pts, color=factor(Team))) + geom_point() +
  theme_minimal()

ggplotly(plot)


```

---
background-image: url(man/figures/practice.jpg)
# Practice time - 15 minutes
Have a go at retrieving some Xpath data from any website. Find a data table and bring it into R with the TableR function.

---
class: inverse, middle, left, hide-logo
# Scraping raw text from websites with xpathTextR function

---

## Extracting text from the NHS Data Dictionary website

```{r texter_extraction}

acc_type <- nhs_data_table %>% 
  filter(link_name=='ACCOMMODATION TYPE')
#browseURL(acc_type$full_url)

# Specify the custom element obtained from the inspect function
xpath <- '//*[@id="element_accommodation_type"]/div/p'
result_list <- NHSDataDictionaRy::xpathTextR(url=acc_type$full_url, 
                              xpath = xpath)

str(result_list)
                              

```

---
## Cleaning the extracted text
The next stage is to clean the extracted text from the list element **result**:

```{r cleaning_the_text}
clean_txt <- trimws(unlist(result_list$result)) %>% 
 

print(clean_txt)


```

Now you could use the inbuilt string functions in the package to strip the text further:

```{r trimming_text}
# Extracted string
NHSDataDictionaRy::left_xl(clean_txt, 20)

```

---
background-image: url(man/figures/practice.jpg)
# Practice time - 10 minutes
Have a go at finding a component from the NHS Data Dictionary, or other website, to test out this functionality.

---

# Scraping a list of links from a website
The NHSDataDictionaRy tool provides functionality for scraping a webpage to extract all the urls from the page. This is how it can be implemented:

```{r implementing_link_scraper}


url <- "https://datadictionary.nhs.uk/data_sets_overview.html"
results <- NHSDataDictionaRy::linkScrapeR(url, SSL_needed = FALSE)
head(results %>% 
       dplyr::select(url), 10)


```

---

# Capstone Project - Working through the whole solution

We will now utilise the tool to scrape the levels of COVID-19 by country, as I know it has been a hard year and we are getting there:

## Data Preparation

```{r nhs_r_comm_site}
xpath <- '//*[@id="main_table_countries_today"]'
url <- "https://www.worldometers.info/coronavirus/"

covid19 <- NHSDataDictionaRy::tableR(url=url, xpath=xpath) %>% 
  select(-c("#"))

c19_reduced <- covid19 %>% 
  select(TotalCases, TotalRecovered, TotalDeaths, TotalTests, NewCases, `Country,Other`, Continent)
# Map through the data frame and strip out the comman from the text and make numeric
```

---

## Data Preparation (continued)

The data returns character strings, so I will use the first 4 columns from my selection above:

```{r data_prep}
c19_numerics <- c19_reduced[,1:5] %>% 
  purrr::map_dfr(~ as.numeric(gsub(",", "", .))) %>% #Get rid of 
  purrr::map_dfr(~as.numeric(gsub("+", "", .))) %>% # Get rid of + symbol
  #Fill in NAs if they are zero
  purrr::map_dfr(~ ifelse(is.na(.),0, .))

```

We then ***purrr::map_dfr*** over the data frame to convert to numeric and strip out the commas and then to fill in the NA values with a zero if they are missing. 


---
## Data Preparation (continued)

Finally, I will join the data back on to the original frame to bring back in the country:

```{r bring_in_country}
c19_final <- cbind(
  c19_reduced <- c19_reduced %>% dplyr::select(`Country,Other`, Continent),
  c19_numerics
) %>% 
  dplyr::rename(Country=`Country,Other`) %>% 
  dplyr::filter(Country!="",
                !str_detect(Country, "Total"), 
                Country!="World")


```

Here we:

- Used cbind base R function to combine the reduced dataset and the numerical dataframe
- We renamed the Country field
- We then filtered out the blank countries and those where there is a total column

---
## The final dataset
This is the final dataset ready to be worked on with R:

```{r the_final_ds}
print(head(c19_final,5))

```
  
With this dataset we will create some visuals. We could even store these in a data table with a time stamp over time to get the relevant days COVID-19 results.

---
## Visualising the results

We'll use our final data frame that we generated to create some visuals of the COVID rates, perhaps a scatter chart would be good for this purpose to see the ratio of total cases to total deaths.

```{r visualise}

highest_cases <- c19_final %>% 
  dplyr::arrange(desc(TotalCases)) %>% 
  dplyr::filter(Country!=Continent) 

```

On the next slide we will visualise the results:

---

## Visualising the results

```{r graph_the_results,out.width='30%'}
options(scipen=999)
ggplot(highest_cases, aes(TotalDeaths,TotalCases)) + 
 geom_point(aes(color=factor(Country))) + theme_minimal() +
  theme(legend.position = "none") +
  labs(x="Total COVID-19 Deaths", y="Total COVID-19 Cases")

```

---
class: inverse, middle, left, hide-logo
# Questions?






